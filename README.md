# Garbage in, Garbage Out: A Case Study on Defective Product Prediction in Manufacturing
The table presents the performance evaluation of various machine learning approaches on the dataset. For each approach, we provide the initial training parameters used during model training. K-fold cross-validation with varying values of k was employed to evaluate the models' effectiveness. The table includes the number of folds (k) utilized for each cross-validation run, along with the corresponding accuracy, precision, and recall results obtained for each fold. The Voting Classifier results are also included, showcasing the final prediction on the test dataset, achieved by combining predictions from five different models: Random Forest Classifier, XGBoost Classifier, Naive Bayes, KNN, and AdaBoost Classifier. The hard-voting method, with equal weight for each classifier's prediction, was used for the final prediction. 
| Machine Learning Approach |  Initial Training Parameters                                                                                                                                                                                                    | Number of Folds (k) |  Cross-Validation Results                                                                                                                                                                                                                                                     |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|  |
|  |
| Random Forest             | criterion='entropy',                    n_estimators=300                                                                                                                                                                        | 6                   | Fold 1: Accuracy: 92.04%    F1: 42.54% <br />Fold 2: Accuracy: 91.23%    F1: 32.04% <br />Fold 3: Accuracy: 91.69%    F1: 37.82%<br />Fold 4: Accuracy: 92.73%    F1: 46.73%<br />Fold 5: Accuracy: 91.61%    F1: 45.65%<br />Fold 6: Accuracy: 91.54%    F1:38.78%  |
|  |
|  |
| XGBoost                   | colsample_bytree=1.0,           learning_rate=0.1,                              gamma=1,                                       max_depth=7,                           n_estimators=300,                           subsample=1.0 | 6                   |  Fold 1: Accuracy: 92.42%    F1: 44.60% <br />Fold 2: Accuracy: 91.16%    F1: 32.65% <br />Fold 3: Accuracy: 91.62%    F1: 38.65% <br />Fold 4: Accuracy: 92.83%    F1: 45.50% <br /> Fold 5: Accuracy: 92.34%    F1: 44.00% <br />Fold 6: Accuracy: 91.31%    F1: 38.48% |
|  |
|  |
| AdaBoost                  | algorithm='SAMME',                   estimator=DecisionTreeClassifier(splitter='random'),<br>                   learning_rate=0.0001, n_estimators=100, random_state=7                                                          | 6                   |  Fold 1: Accuracy: 82.59%    F1: 27.21% <br />Fold 2: Accuracy: 87.26%    F1: 29.89% <br />Fold 3: Accuracy: 88.44%    F1: 37.86% <br />Fold 4: Accuracy: 89.00%    F1: 44.73% <br />Fold 5: Accuracy: 87.63%    F1: 40.73% <br />Fold 6: Accuracy: 87.06%    F1: 36.55% |
|  |
|  |
| Naive Bayes               | var_smoothing=1e-08                                                                                                                                                                                                             | 6                   |  Fold 1: Accuracy: 87.08%    F1: 36.78% <br />Fold 2: Accuracy: 85.09%    F1: 31.76% <br />Fold 3: Accuracy: 85.97%    F1: 33.86% <br />Fold 4: Accuracy: 88.56%    F1: 37.33% <br />Fold 5: Accuracy: 79.33%    F1: 35.09 <br />Fold 6: Accuracy: 83.31%    F1: 31.01% |
|  |
|  |
| K-Nearest Neighbors       | n_neighbors=3,                                               p=1,                                                  weights='distance'                                                                                           | 6                   |  Fold 1: Accuracy: 87.60%    F1: 30.81% <br />Fold 2: Accuracy: 87.68%    F1: 28.47% <br />Fold 3: Accuracy: 89.25%    F1: 29.43% <br />Fold 4: Accuracy: 90.25%    F1: 32.30% <br />Fold 5: Accuracy: 88.75%    F1: 30.13% <br />Fold 6: Accuracy: 89.52%    F1: 27.89% |
|  |
|  |
